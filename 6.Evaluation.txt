from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import itertools
import numpy as np
import matplotlib.pyplot as plt

#Predict on Test Set
test_predicts = []
test_targets = []

for x_batch, y_batch in testDataset:
    y_pred = student.predict(x_batch)
    test_predicts.extend(np.argmax(y_pred, axis=-1))
    test_targets.extend(np.argmax(y_batch, axis=-1))

# Calculate Accuracy, Precision, and Recall
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average="weighted")
recall = recall_score(y_true, y_pred, average="weighted")
f1 = f1_score(y_true, y_pred, average="weighted")
print(f"Accuracy: {accuracy*100:.2f}%")
print(f"Precision: {precision*100:.2f}%")
print(f"Recall: {recall*100:.2f}%")
print(f"F1-Score: {f1*100:.2f}%")

# Visualize the Confusion Matrix
confusion = confusion_matrix(test_targets, test_predicts)

plt.figure(figsize=(12, 10))
plt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(classDirs))
plt.xticks(tick_marks, classDirs, rotation=90, ha="right")
plt.yticks(tick_marks, classDirs)
plt.grid(which='major', axis='both', linestyle='-', color='k', linewidth=0.1)
plt.gca().set_xticks([x-0.5 for x in plt.gca().get_xticks()][1:], minor='true')
plt.gca().set_yticks([y-0.5 for y in plt.gca().get_yticks()][1:], minor='true')

thresh = confusion.max() / 2.
for i, j in itertools.product(range(confusion.shape[0]), range(confusion.shape[1])):
    plt.text(j, i, format(confusion[i, j], 'd'),
             horizontalalignment="center",
             color="white" if confusion[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
