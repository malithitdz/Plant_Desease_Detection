import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import time
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.applications import DenseNet121, DenseNet201
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers
import keras_tuner as kt


# Model setup
num_classes = len(classCounts)
best_dropout = 0.2
best_l2 = 2.7928158109277624e-05
best_optimizer = "adam"

# Load the teacher model
teacher_path = 'DenseNet121.h5'
teacher = tf.keras.models.load_model(teacher_path)

# Knowledge distillation setup
class Distiller(tf.keras.Model):
    def __init__(self, student, teacher):
        super(Distiller, self).__init__()
        self.teacher = teacher
        self.student = student

    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3):
        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)
        self.student_loss_fn = student_loss_fn
        self.distillation_loss_fn = distillation_loss_fn
        self.alpha = alpha
        self.temperature = temperature

    def train_step(self, data):
        x, y = data
        teacher_predictions = self.teacher(x, training=False)
        with tf.GradientTape() as tape:
            student_predictions = self.student(x, training=True)
            student_loss = self.student_loss_fn(y, student_predictions)
            distillation_loss = self.distillation_loss_fn(
                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),
                tf.nn.softmax(student_predictions / self.temperature, axis=1)
            )
            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss

        trainable_vars = self.student.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        self.compiled_metrics.update_state(y, student_predictions)
        results = {m.name: m.result() for m in self.metrics}
        results.update({"student_loss": student_loss, "distillation_loss": distillation_loss})
        return results

    def test_step(self, data):
        x, y = data
        y_pred = self.student(x, training=False)
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}

def build_student():
    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(imgHeight, imgWidth, 3))
    for layer in base_model.layers:
        layer.trainable = False
    student = tf.keras.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dropout(best_dropout),
        layers.Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(best_l2))
    ])
    return student

def build_distiller(hp):
    alpha = hp.Float('alpha', min_value=0.1, max_value=0.9, step=0.1)
    temperature = hp.Float('temperature', min_value=1.0, max_value=10.0, step=1.0)
    student = build_student()
    distiller = Distiller(student=student, teacher=teacher)
    distiller.compile(
        optimizer=best_optimizer,
        metrics=['accuracy'],
        student_loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
        distillation_loss_fn=tf.keras.losses.KLDivergence(),
        alpha=alpha,
        temperature=temperature
    )
    return distiller

tuner = kt.BayesianOptimization(
    build_distiller,
    objective=kt.Objective("distillation_loss", direction="min"),
    max_trials=10,
    executions_per_trial=2,
    directory='keras_tuner_dir',
    project_name='knowledge_distillation_tuning'
)

tuner.search(trainDataset, validation_data=valDataset, epochs=10)

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"Best alpha: {best_hps.get('alpha')}")
print(f"Best temperature: {best_hps.get('temperature')}")

best_model = tuner.get_best_models(num_models=1)[0]

checkpointCallback = ModelCheckpoint('student_modelMobileNetV2.h5', monitor='val_loss', save_best_only=True)
lrReducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)
earlyStopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

start_time = time.time()
history = best_model.fit(trainDataset, validation_data=valDataset, epochs=5, callbacks=[checkpointCallback, lrReducer, earlyStopping])
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Training took {elapsed_time:.2f} seconds.")
